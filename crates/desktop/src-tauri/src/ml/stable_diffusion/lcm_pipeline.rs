use std::path::PathBuf;
use std::sync::{Arc, RwLock, RwLockReadGuard, RwLockWriteGuard};

use crate::events::notification_event::{NotificationEvent, NotificationModelType};
use crate::ml::image::dynamic_image_to_tensor::dynamic_image_to_tensor;
use crate::ml::image::tensor_to_image_buffer::{tensor_to_image_buffer, RgbImage};
use crate::ml::model_cache::ModelCache;
use crate::ml::model_file::{ModelFile, StableDiffusionVersion};
use crate::ml::prompt_cache::PromptCache;
use crate::ml::stable_diffusion::get_vae_scale::get_vae_scale;
use crate::ml::stable_diffusion::infer_clip_text_embeddings::infer_clip_text_embeddings;
use crate::ml::stable_diffusion::remap_lcm_strength_range::remap_lcm_strength_range;
use crate::ml::weights_registry::weights::{LYKON_DEAMSHAPER_7_TEXT_ENCODER_FP16, LYKON_DEAMSHAPER_7_VAE, SIMIANLUO_LCM_DREAMSHAPER_V7_UNET};
use crate::state::app_config::AppConfig;
use crate::state::app_dir::AppDataRoot;
use anyhow::{anyhow, Error as E, Result};
use candle_core::{DType, IndexOp, Tensor, D};
use candle_nn::VarBuilder;
use candle_transformers::models::stable_diffusion::lcm::LCMScheduler;
use candle_transformers::models::stable_diffusion::unet_2d::BlockConfig;
use candle_transformers::models::stable_diffusion::unet_2d::UNet2DConditionModel;
use candle_transformers::models::stable_diffusion::unet_2d::UNet2DConditionModelConfig;
use candle_transformers::models::stable_diffusion::vae::{AutoEncoderKL, DiagonalGaussianDistribution};
use image::DynamicImage;
use log::info;
use rand::Rng;
use tauri::{AppHandle, Emitter};

const DEFAULT_STRENGTH : f64 = 75.0;

pub struct Args<'a> {
  pub image: &'a DynamicImage,
  pub prompt: String,
  pub uncond_prompt: String,
  pub cfg_scale: Option<f64>,
  pub i2i_strength: Option<f64>,
  pub configs: &'a AppConfig,
  pub model_cache: &'a ModelCache,
  pub prompt_cache: &'a PromptCache,
  pub app_data_root: &'a AppDataRoot,
  pub app: &'a AppHandle,
  pub use_flash_attn: bool,
}

pub fn lcm_pipeline(args: Args<'_>) -> Result<RgbImage> {
  let Args { prompt, uncond_prompt, cfg_scale, i2i_strength, configs, model_cache, prompt_cache, app, image, app_data_root, use_flash_attn } = args;

  let weights_dir = app_data_root.weights_dir();
  
  let img2img_strength = args.i2i_strength
    .map(|s| remap_lcm_strength_range(s))
    .unwrap_or(DEFAULT_STRENGTH);
  
  let img2img_strength = img2img_strength / 100.0;

  let use_f16 = true;

  // Use LCM Scheduler instead of Euler Ancestral for better speed and quality
  let mut scheduler = LCMScheduler::new(configs.scheduler_steps, img2img_strength, candle_transformers::models::stable_diffusion::lcm::LCMSchedulerConfig::default())?;

  let seed = configs.seed.unwrap_or_else(|| rand::thread_rng().gen());
  configs.device.set_seed(seed)?;

  let guidance_scale = match cfg_scale {
    Some(guidance_scale) => guidance_scale,
    None => match configs.sd_version {
      StableDiffusionVersion::V1_5 | StableDiffusionVersion::V2_1 | StableDiffusionVersion::Xl => 7.5,
      StableDiffusionVersion::Turbo => 0.,
      _ => 0., // NB: Not sure what the other model families should use, so sticking with "0"
    },
  };

  // let use_guide_scale = guidance_scale > 1.0;
  let use_guide_scale = false;

  let maybe_cached = prompt_cache.get_copy(&prompt)?;

  let mut text_embeddings = if let Some(tensor) = maybe_cached {
    tensor
  } else {
    info!("Loading clip weights...");

    let clip_weights = weights_dir.weight_path(&LYKON_DEAMSHAPER_7_TEXT_ENCODER_FP16);
    let clip_weights = Some(clip_weights.to_string_lossy().to_string());

    info!("Prompt is NOT cached! Calculating embedding...");
    let tensor = infer_clip_text_embeddings(
      &prompt,
      &uncond_prompt,
      None, // tokenizer
      clip_weights,
      None, // clip2_weights
      configs.sd_version,
      &configs.sd_config,
      use_f16, // use_f16
      &configs.device,
      configs.dtype,
      use_guide_scale,
      weights_dir,
    )?;
    prompt_cache.store_copy(&prompt, &tensor)?;
    tensor
  };

  println!("Text embeddings shape: {:?}", text_embeddings.shape());

  println!("Loading input image into tensor...");

  let input_image = dynamic_image_to_tensor(image, &configs.device, configs.dtype)?;

  println!("Reference image shape: {:?}", input_image.shape());

  let vae_scale = get_vae_scale(configs.sd_version);

  let maybe_vae = model_cache.get_vae()?;

  let vae = match maybe_vae {
    Some(vae) => vae,
    None => {
      let vae_file = weights_dir.weight_path(&LYKON_DEAMSHAPER_7_VAE);

      println!("Building VAE model from file {:?}...", &vae_file);

      let vae = configs.sd_config.build_vae(vae_file, &configs.device, configs.dtype)?;

      let vae = Arc::new(vae);

      model_cache.set_vae(vae.clone())?;

      vae
    }
  };

  let maybe_unet = model_cache.get_unet()?;

  let unet = match maybe_unet {
    Some(unet) => unet,
    None => {
      info!("No unet found in cache; loading...");

      let unet_weights = weights_dir.weight_path(&SIMIANLUO_LCM_DREAMSHAPER_V7_UNET);

      let in_channels = match configs.sd_version {
        StableDiffusionVersion::XlInpaint | StableDiffusionVersion::V2Inpaint | StableDiffusionVersion::V1_5Inpaint => 9,
        _ => 4,
      };

      // For LCM models, ensure that:
      // 1. We're using the correct prediction type (epsilon)
      // 2. The cross-attention dimension is correct (768 for Dreamshaper)
      // 3. The in_channels matches the expected input (4 for standard, 9 for inpainting)
      let unet_config = UNet2DConditionModelConfig {
        // LCM specific config values
        cross_attention_dim: 768,      // Dreamshaper v7 uses 768
        time_cond_proj_dim: Some(256), // LCM models use 256-dimensional guidance embeddings
        // Rest of configuration...
        center_input_sample: false,
        flip_sin_to_cos: true,
        freq_shift: 0.0,
        blocks: vec![
          BlockConfig { out_channels: 320, use_cross_attn: Some(1), attention_head_dim: 8 },
          BlockConfig { out_channels: 640, use_cross_attn: Some(1), attention_head_dim: 8 },
          BlockConfig { out_channels: 1280, use_cross_attn: Some(1), attention_head_dim: 8 },
          BlockConfig { out_channels: 1280, use_cross_attn: None, attention_head_dim: 8 }
        ],
        layers_per_block: 2,
        downsample_padding: 1,
        mid_block_scale_factor: 1.0,
        norm_num_groups: 32,
        norm_eps: 1e-5,
        sliced_attention_size: None,
        use_linear_projection: false,
        time_embed_dim: Some(1280),
        // time_embed_dim: Some(256),
      };

      // Load the model directly from the safetensors file
      let vs_unet = unsafe { VarBuilder::from_mmaped_safetensors(&[unet_weights], configs.dtype, &configs.device)? };

      let unet = UNet2DConditionModel::new(vs_unet, in_channels, 4, use_flash_attn, unet_config)?;

      let unet = Arc::new(unet);

      model_cache.set_unet(unet.clone())?;

      unet
    },
  };

  let init_latent_dist: DiagonalGaussianDistribution = vae.encode(&input_image)?;

  // Create LCM guidance scale embeddings
  let embedding_dim = 1280;
  let guidance_scale_embedding = scheduler.get_guidance_scale_embedding(guidance_scale, embedding_dim, &configs.device, configs.dtype)?;

  // Generate latents from input image using the LCM approach
  println!("Generating latents from input image...");
  let init_latents = (init_latent_dist.sample()? * vae_scale)?;
  println!("Initial latents shape: {:?}", init_latents.shape());

  // Calculate timesteps for LCM with proper img2img handling
  let timesteps = LCMScheduler::get_timesteps_for_steps(configs.scheduler_steps, img2img_strength);
  let t_start = timesteps[0]; // First denoising step

  // Add noise at the appropriate timestep for img2img
  println!("Adding noise to latents for t_start={}", t_start);
  let noise = init_latents.randn_like(0f64, 1f64)?;
  let latents = scheduler.add_noise(&init_latents, noise, t_start)?;

  let mut latents = latents;
  let timesteps: Vec<_> = scheduler.timesteps().iter().copied().collect();

  for (timestep_index, &timestep) in timesteps.iter().enumerate() {
    println!("Processing step {}/{} @ timestamp = {}", timestep_index + 1, timesteps.len(), timestep);

    let latent_model_input = if use_guide_scale { Tensor::cat(&[&latents, &latents], 0)? } else { latents.clone() };

    let latent_model_input = scheduler.scale_model_input(latent_model_input, timestep);

    // Use the guidance scale embedding in the UNet inference with the proper method name
    let mut noise_pred = match unet.forward_with_guidance(&latent_model_input, timestep as f64, &text_embeddings, Some(&guidance_scale_embedding)) {
      Ok(pred) => pred,
      Err(e) => {
        println!("UNet inference failed with error: {}", e);
        return Err(anyhow::anyhow!("UNet inference failed: {}", e));
      },
    };

    if use_guide_scale {
      let chunks = noise_pred.chunk(2, 0)?;
      let (noise_pred_uncond, noise_pred_text) = (&chunks[0], &chunks[1]);
      noise_pred = (noise_pred_uncond + (guidance_scale * (noise_pred_text - noise_pred_uncond)?)?)?;
    }

    // Apply the LCM scheduler step
    latents = scheduler.step(&noise_pred, timestep, &latents, timestep_index, configs.scheduler_steps)?;
  }

  println!("Diffusion process completed, decoding image...");
  let image = vae.decode(&(latents / vae_scale)?)?;

  println!("VAE decode completed");

  println!("Scaling image back");
  let image = ((image / 2.)? + 0.5)?;

  println!("Normalized image values");
  let image = (image.clamp(0f32, 1.)? * 255.)?;

  println!("Convert to int8");
  let image = image.to_dtype(DType::U8)?;

  println!("Converted to 8-bit format");

  let image = tensor_to_image_buffer(&image.i(0)?)?;

  Ok(image)
}
