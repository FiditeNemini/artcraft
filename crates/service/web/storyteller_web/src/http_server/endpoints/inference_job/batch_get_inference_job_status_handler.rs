use std::collections::HashSet;
use std::fmt;
use std::sync::Arc;

use actix_web::{HttpMessage, HttpRequest, HttpResponse, web};
use actix_web::error::ResponseError;
use actix_web::http::StatusCode;
use actix_web::web::Path;
use actix_web_lab::extract::Query;
use chrono::{DateTime, Utc};
use log::error;
use r2d2_redis::redis::{Commands, RedisResult};
use utoipa::{IntoParams, ToSchema};

use buckets::public::media_files::bucket_file_path::MediaFileBucketPath;
use buckets::public::voice_conversion_results::bucket_file_path::VoiceConversionResultOriginalFilePath;
use enums::by_table::generic_inference_jobs::frontend_failure_category::FrontendFailureCategory;
use enums::by_table::generic_inference_jobs::inference_category::InferenceCategory;
use enums::common::job_status_plus::JobStatusPlus;
use mysql_queries::queries::generic_inference::web::batch_get_inference_job_status::{batch_get_inference_job_status, GenericInferenceJobStatus};
use redis_common::redis_keys::RedisKeys;
use tokens::tokens::generic_inference_jobs::InferenceJobToken;
use tokens::tokens::media_files::MediaFileToken;
use crate::http_server::endpoints::media_files::get::batch_get_media_files_handler::BatchGetMediaFilesQueryParams;
use crate::http_server::responses::filter_model_name::maybe_filter_model_name;

use crate::http_server::web_utils::response_error_helpers::to_simple_json_error;
use crate::server_state::ServerState;

/// For certain jobs or job classes (eg. non-premium), we kill the jobs if the user hasn't
/// maintained a keepalive. This prevents wasted work when users who are unlikely to return
/// navigate away. Premium users have accounts and can always return to the site, so they
/// typically do not require keepalive.
const JOB_KEEPALIVE_TTL_SECONDS : usize = 60 * 3;


#[derive(Deserialize, ToSchema, IntoParams)]
pub struct BatchGetInferenceJobStatusQueryParams {
  /// A grab bag of tokens of various types.
  /// Technically we should pair with token types, as that's the unique index.
  /// But since each token family generally has its own prefix, this should be fine.
  ///
  /// NB: We're using actix_web_lab's Query<T>, because the default actix_web Query<T> doesn't support URL
  /// decoding sequences yet.
  /// See https://github.com/actix/actix-web/issues/1301
  ///
  pub tokens: HashSet<String>,
}

#[derive(Serialize, ToSchema)]
pub struct BatchGetInferenceJobStatusSuccessResponse {
  pub success: bool,
  pub job_states: Vec<BatchInferenceJobStatusResponsePayload>,
}

#[derive(Serialize, ToSchema)]
pub struct BatchInferenceJobStatusResponsePayload {
  pub job_token: InferenceJobToken,

  pub request: BatchRequestDetailsResponse,
  pub status: BatchStatusDetailsResponse,
  pub maybe_result: Option<BatchResultDetailsResponse>,

  pub created_at: DateTime<Utc>,
  pub updated_at: DateTime<Utc>,
}

/// Details about what the user requested for generation
#[derive(Serialize, ToSchema)]
pub struct BatchRequestDetailsResponse {
  pub inference_category: InferenceCategory,
  pub maybe_model_type: Option<String>,
  pub maybe_model_token: Option<String>,
  /// Title of the model, if it has one
  pub maybe_model_title: Option<String>,

  /// If the result was TTS, this is the raw inference text.
  pub maybe_raw_inference_text: Option<String>,
}

/// Details about the ongoing job status
#[derive(Serialize, ToSchema)]
pub struct BatchStatusDetailsResponse {
  /// Primary status from the database (a state machine).
  pub status: JobStatusPlus,

  /// Extra, temporary status from Redis.
  /// This can denote inference progress, and the Python code can write to it.
  pub maybe_extra_status_description: Option<String>,

  pub maybe_assigned_worker: Option<String>,
  pub maybe_assigned_cluster: Option<String>,

  pub maybe_first_started_at: Option<DateTime<Utc>>,

  pub attempt_count: u8,

  /// Whether the frontend needs to maintain a keepalive check.
  /// This is typically only for non-premium users.
  pub requires_keepalive: bool,

  /// An enum the frontend can use to display localized/I18N error
  /// messages. These pertain to both transient and permanent failures.
  pub maybe_failure_category: Option<FrontendFailureCategory>
}

/// Details about the completed result (if any)
#[derive(Serialize, ToSchema)]
pub struct BatchResultDetailsResponse {
  pub entity_type: String,
  pub entity_token: String,

  /// NB: This is only for audio- or video- type results.
  pub maybe_public_bucket_media_path: Option<String>,

  pub maybe_successfully_completed_at: Option<DateTime<Utc>>,
}

#[derive(Debug, ToSchema)]
pub enum BatchGetInferenceJobStatusError {
  ServerError,
  NotFound,
}

impl ResponseError for BatchGetInferenceJobStatusError {
  fn status_code(&self) -> StatusCode {
    match *self {
      BatchGetInferenceJobStatusError::ServerError => StatusCode::INTERNAL_SERVER_ERROR,
      BatchGetInferenceJobStatusError::NotFound => StatusCode::NOT_FOUND,
    }
  }

  fn error_response(&self) -> HttpResponse {
    let error_reason = match self {
      Self::ServerError => "server error".to_string(),
      Self::NotFound => "not found".to_string(),
    };

    to_simple_json_error(&error_reason, self.status_code())
  }
}

// NB: Not using derive_more::Display since Clion doesn't understand it.
impl fmt::Display for BatchGetInferenceJobStatusError {
  fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
    write!(f, "{:?}", self)
  }
}


#[utoipa::path(
  get,
  tag = "Jobs",
  path = "/v1/jobs/batch",
  responses(
    (status = 200, body = BatchGetInferenceJobStatusSuccessResponse),
    (status = 500, body = BatchGetInferenceJobStatusError),
  ),
  params(
    BatchGetInferenceJobStatusQueryParams
  )
)]
pub async fn batch_get_inference_job_status_handler(
  http_request: HttpRequest,
  query: Query<BatchGetInferenceJobStatusQueryParams>,
  server_state: web::Data<Arc<ServerState>>) -> Result<HttpResponse, BatchGetInferenceJobStatusError>
{
  let tokens = query.tokens.iter()
      .map(|token| token.trim())
      // NB: A bunch of Python clients use our API and can fail in this manner.
      // This was a large traffic driver during the 2023-03-08 outage.
      // Presumably it's this client: https://github.com/shards-7/fakeyou.py
      .filter(|token| !token.is_empty() && *token != "None")
      .map(|token| InferenceJobToken::new_from_str(token))
      .collect::<Vec<_>>();

  if tokens.is_empty() {
    return records_to_response(Vec::new());
  }

  // NB: Since this is publicly exposed, we don't query sensitive data.
  let maybe_records = batch_get_inference_job_status(
    &tokens,
    &server_state.mysql_pool
  ).await;

  let records = match maybe_records {
    Ok(records) => records,
    Err(err) => {
      error!("tts job query error: {:?}", err);
      return Err(BatchGetInferenceJobStatusError::ServerError);
    }
  };

  let mut redis = server_state.redis_pool
      .get()
      .map_err(|e| {
        error!("redis error: {:?}", e);
        BatchGetInferenceJobStatusError::ServerError
      })?;

  // TODO(bt,2024-04-22): Look up the extra redis statuses per item.

  let keepalive_keys = records.iter()
      .filter(|record| record.is_keepalive_required)
      .map(|record| RedisKeys::generic_inference_keepalive(record.job_token.as_str()))
      .collect::<Vec<_>>();

  for key in keepalive_keys.iter() {
    // TODO(bt,2024-04-22): There is no msetex. We'll need to run a Redis pipeline here.
    //  https://stackoverflow.com/questions/16423342/redis-multi-set-with-a-ttl
    let _: Option<String> = match redis.set_ex(key, "1", JOB_KEEPALIVE_TTL_SECONDS) {
      Ok(Some(status)) => Some(status),
      Ok(None) => None,
      Err(e) => {
        error!("redis error setting job keepalive: {:?}", e);
        None // Fail open (which in this case is bad! it will kill jobs if cluster has many jobs / is slow!)
      },
    };
  }

  records_to_response(records)
}

fn records_to_response(records: Vec<GenericInferenceJobStatus>) -> Result<HttpResponse, BatchGetInferenceJobStatusError> {
  let records = records.into_iter()
      .map(|record| {
        db_record_to_response_payload(record, None)
      })
      .collect::<Vec<_>>();

  let response = BatchGetInferenceJobStatusSuccessResponse {
    success: true,
    job_states: records,
  };

  let body = serde_json::to_string(&response)
      .map_err(|e| {
        error!("error returning response: {:?}",  e);
        BatchGetInferenceJobStatusError::ServerError
      })?;

  Ok(HttpResponse::Ok()
      .content_type("application/json")
      .body(body))
}

fn db_record_to_response_payload(
  record: GenericInferenceJobStatus,
  maybe_extra_status_description: Option<String>,
) -> BatchInferenceJobStatusResponsePayload {
  let inference_category = record.request_details.inference_category;

  BatchInferenceJobStatusResponsePayload {
    job_token: record.job_token,
    request: BatchRequestDetailsResponse {
      inference_category: record.request_details.inference_category,
      maybe_model_type: maybe_filter_model_name(record.request_details.maybe_model_type.as_deref()),
      maybe_model_token: record.request_details.maybe_model_token,
      maybe_model_title: record.request_details.maybe_model_title,
      maybe_raw_inference_text: record.request_details.maybe_raw_inference_text,
    },
    status: BatchStatusDetailsResponse {
      status: record.status,
      maybe_extra_status_description,
      maybe_assigned_worker: maybe_filter_model_name(record.maybe_assigned_worker.as_deref()),
      maybe_assigned_cluster: record.maybe_assigned_cluster,
      maybe_first_started_at: record.maybe_first_started_at,
      attempt_count: record.attempt_count as u8,
      requires_keepalive: record.is_keepalive_required,
      maybe_failure_category: record.maybe_frontend_failure_category,
    },
    maybe_result: record.maybe_result_details.map(|result_details| {
      // NB: Be careful here, because this varies based on the type of inference result.
      let public_bucket_media_path = match inference_category {
        InferenceCategory::LipsyncAnimation => {
          MediaFileBucketPath::from_object_hash(
            &result_details.public_bucket_location_or_hash,
            result_details.maybe_media_file_public_bucket_prefix.as_deref(),
            result_details.maybe_media_file_public_bucket_extension.as_deref())
              .get_full_object_path_str()
              .to_string()
        }
        InferenceCategory::Mocap => {
          MediaFileBucketPath::from_object_hash(
            &result_details.public_bucket_location_or_hash,
            result_details.maybe_media_file_public_bucket_prefix.as_deref(),
            result_details.maybe_media_file_public_bucket_extension.as_deref())
              .get_full_object_path_str()
              .to_string()
        }
        InferenceCategory::VideoFilter => {
          MediaFileBucketPath::from_object_hash(
            &result_details.public_bucket_location_or_hash,
            result_details.maybe_media_file_public_bucket_prefix.as_deref(),
            result_details.maybe_media_file_public_bucket_extension.as_deref())
              .get_full_object_path_str()
              .to_string()
        }
        InferenceCategory::Workflow => {
          MediaFileBucketPath::from_object_hash(
            &result_details.public_bucket_location_or_hash,
            result_details.maybe_media_file_public_bucket_prefix.as_deref(),
            result_details.maybe_media_file_public_bucket_extension.as_deref())
              .get_full_object_path_str()
              .to_string()
        }
        InferenceCategory::TextToSpeech => {
          match result_details.entity_type.as_str() {
            "media_file" => {
              // NB: We're migrating TTS to media_files.
              // Zero shot TTS uses media files.
              // Legacy TT2 uses old pathing.
              MediaFileBucketPath::from_object_hash(
                &result_details.public_bucket_location_or_hash,
                result_details.maybe_media_file_public_bucket_prefix.as_deref(),
                result_details.maybe_media_file_public_bucket_extension.as_deref())
                  .get_full_object_path_str()
                  .to_string()
            }
            _ => {
              // NB: TTS results receive the legacy treatment where their table only reports the full bucket path
              result_details.public_bucket_location_or_hash
            }
          }
        }
        InferenceCategory::VoiceConversion => {
          match result_details.entity_type.as_str() {
            "media_file" => {
              // NB: We're migrating voice conversion to media_files.
              MediaFileBucketPath::from_object_hash(
                &result_details.public_bucket_location_or_hash,
                result_details.maybe_media_file_public_bucket_prefix.as_deref(),
                result_details.maybe_media_file_public_bucket_extension.as_deref())
                  .get_full_object_path_str()
                  .to_string()
            }
            _ => {
              // NB: This is the old voice conversion result pathing.
              VoiceConversionResultOriginalFilePath::from_object_hash(&result_details.public_bucket_location_or_hash)
                  .get_full_object_path_str()
                  .to_string()
            }
          }
        }
        InferenceCategory::ImageGeneration => { 
          "".to_string()
        }
        InferenceCategory::FormatConversion => {
          "".to_string()
        }
        InferenceCategory::ConvertBvhToWorkflow => {
          "".to_string()
        }
      };
     
      BatchResultDetailsResponse {
        entity_type: result_details.entity_type,
        entity_token: result_details.entity_token,
        maybe_public_bucket_media_path: Some(public_bucket_media_path),
        maybe_successfully_completed_at: result_details.maybe_successfully_completed_at,
      }
    }),
    created_at: record.created_at,
    updated_at: record.updated_at,
  }
}
