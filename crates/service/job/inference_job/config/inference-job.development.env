# inference-job default .env configurations (these can be overridden with k8s)

# NB: For my crapy internet connection
BUCKET_TIMEOUT_SECONDS=6000

# NB: To not step on storyteller-web (the main API gateway)
HTTP_BIND_ADDRESS=0.0.0.0:11223

# NB: Disable if not testing Redis
REDIS_FOR_KEEPALIVE_URL="redis://localhost/0"

# ================================= #
#                                   #
#            Vall-E-X               #
#                                   #
# ================================= #

# should rename my folder 
# abs path to the folder that contains the inference code need to get that container in the repo need basepath? relative to everyones env
VALL_E_X_INFERENCE_ROOT_DIRECTORY="%storyteller_ml%/tts/VALL-E-X"
#VALL_E_X_INFERENCE_ROOT_DIRECTORY="/home/tensor/code/storyteller/storyteller-ml/tts/VALL-E-X"
VALL_E_X_INFERENCE_COMMAND="python3 main.py"
VALL_E_X_INFERENCE_MAYBE_VENV_COMMAND="source .venv/bin/activate"

# VALL_E_X_INFERENCE_MAYBE_DOCKER_IMAGE="" # fill this out for dev?
#VALL_E_X_INFERENCE_ROOT_DIRECTORY="%storyteller_ml%/tts/VALL-E-X"
#VALL_E_X_INFERENCE_COMMAND="python3 main.py"
#VALL_E_X_INFERENCE_MAYBE_VENV_COMMAND="source .env/bin/activate"
#VALL_E_X_INFERENCE_MAYBE_DOCKER_IMAGE="" # fill this out for dev?

STYLE_TTS2_INFERENCE_ROOT_DIRECTORY=""
STYLE_TTS2_INFERENCE_COMMAND = ""
STYLE_TTS2_INFERENCE_EXECUTABLE = ""

# ================================= #
#                                   #
#             RVC (v2)              #
#                                   #
# ================================= #

RVC_V2_INFERENCE_ROOT_DIRECTORY="%storyteller_ml%/voice_conversion/rvc-v2-brandon"
RVC_V2_INFERENCE_COMMAND="python fakeyou_infer.py"
RVC_V2_INFERENCE_MAYBE_VENV_COMMAND="source python/bin/activate"

# ================================= #
#                                   #
#            SAD TALKER             #
#                                   #
# ================================= #

SAD_TALKER_INFERENCE_ROOT_DIRECTORY="%storyteller_ml%/animation/SadTalker"
SAD_TALKER_INFERENCE_COMMAND="python inference.py"
SAD_TALKER_INFERENCE_MAYBE_VENV_COMMAND="source python/bin/activate"

# ================================= #
#                                   #
#          FFMPEG WATERMARK         #
#                                   #
# ================================= #

FFMPEG_LOGO_LOGO_PATH="%storyteller_rust%/includes/container_includes/image_assets/fakeyou_watermark.png"

# ================================= #
#                                   #
#           SO-VITS-SVC             #
#                                   #
# ================================= #

# Cache hubert in a recoverable place so do don't constantly re-download it.
HUBERT_PATH="/tmp/hubert/checkpoint_best_legacy_500.pt"

SO_VITS_SVC_INFERENCE_ROOT_DIRECTORY="/models/voice_conversion/so-vits-svc/src" #NB: so-vits-svc-paul-2
SO_VITS_SVC_INFERENCE_MAYBE_DOCKER_IMAGE="1136fe8b50d5" # so-vits-svc-paul-2: --f0-method
SO_VITS_SVC_INFERENCE_COMMAND="/install/python/bin/python3.10 fakeyou_infer.py" # NB: so-vits-svc-paul-2
SO_VITS_SVC_INFERENCE_MAYBE_DEFAULT_CONFIG_PATH="/models/voice_conversion/so-vits-svc/example_config.json"

# ================================= #
#                                   #
#         TACOTRON (LEGACY)         #
#                                   #
# ================================= #

## Old docker image
#TACOTRON2_VOCODES_DOCKER_IMAGE_SHA=1ba08f70f2ba

TT2_LEGACY_ROOT_DIRECTORY="/models/tts"
TT2_LEGACY_MAYBE_PYTHON_INTERPRETER="python3.6"
TT2_LEGACY_MAYBE_DOCKER_IMAGE_SHA="0d29da13e2df"
TT2_AS_MEDIA_FILES="true"

# ================================= #
#                                   #
#              VITS                 #
#                                   #
# ================================= #

VITS_INFERENCE_ROOT_DIRECTORY="/vits"
VITS_INFERENCE_SCRIPT="infer_ts_job.py"
VITS_INFERENCE_MAYBE_PYTHON_INTERPRETER="python3"
VITS_INFERENCE_MAYBE_DOCKER_IMAGE_SHA="03017896367b"

# ================================= #
#                                   #
#            Rerender               #
#                                   #
# ================================= #
RERENDER_INFERENCE_ROOT_DIRECTORY="/home/salt/clone/storyteller-ml/animation/rerender-lora/"
RERENDER_INFERENCE_EXECUTABLE_OR_COMMAND="venv/bin/python rerender.py"

# ================================= #
#                                   #
#          STABLE DIFFUSION         #
#                                   #
# ================================= #
STABLE_DIFFUSION_INFERENCE_ROOT_DIRECTORY="/home/tensor/code/storyteller/storyteller-ml/image_generation/Storyteller-Studio-Filter/"
STABLE_DIFFUSION_INFERENCE_COMMAND=".venv/bin/python main.py"

# ================================= #
#                                   #
#            MocapNET               #
#                                   #
# ================================= #
MOCAPNET_INFERENCE_ROOT_DIRECTORY="/home/salt/clone/storyteller-ml/animation/MocapNET/src/python/mnet4/"
MOCAPNET_INFERENCE_EXECUTABLE_OR_COMMAND="pythonVirtualEnvironment/bin/python -m mediapipeHolisticWebcamMocapNET"

# ================================= #
#                                   #
#            ComfyUI                #
#                                   #
# ================================= #
COMFY_INFERENCE_ROOT_DIRECTORY="/home/salt/clone/storyteller-ml/image_generation/ComfyUI"
COMFY_INFERENCE_EXECUTABLE_OR_COMMAND="venv/bin/python ../ComfyLauncher/ComfyRunner.py"
STYLE_TTS2_INFERENCE_ROOT_DIRECTORY="/home/salt/clone/storyteller-ml/tts/style-tts2"
STYLE_TTS2_INFERENCE_COMMAND="venv/bin/python3.8 main.py"

# ================================= #
#                                   #
#          CACHE SETTINGS           #
#                                   #
# ================================= #

# Huggingface caches data at ~/.cache/huggingface (/root/cache/huggingface),
# which isn't the shared volume and will get blown away.
# HF_HOME="/tmp/huggingface"
HF_DATASETS_CACHE="/tmp/huggingface"

# nltk also caches data.
NLTK_DATA="/tmp/nltk_data"

